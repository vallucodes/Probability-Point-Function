# High-Performance Inverse Normal Distribution (Probit Function)

Implementation of the inverse cumulative normal distribution function (Î¦â»Â¹), achieving **~17Ã— speedup** over naive bisection and **~6Ã— additional speedup** through parallelization.

## ğŸ¯ What This Does

The **inverse normal distribution** (also called the *probit function* or *percent-point function*) answers the question: *"Given a probability, what value corresponds to it in a normal distribution?"*

### Visual Example

For a normal distribution with Î¼=64.53 and Ïƒ=3.05, if you want to find the value where 75% of data falls below it:
- **Input**: probability = 0.75
- **Output**: x = 66.59

<img src="imgs/normal.png" alt="Normal Distribution" width="500">
<img src="imgs/PPF.png" alt="Percent Point Function" width="500">

This function is essential in statistics, finance (risk modeling), machine learning (sampling), and scientific computing.

## âš¡ Why This Implementation?

| Aspect | Traditional Bisection | This Implementation |
|--------|----------------------|---------------------|
| **Method** | Iterative search (~80 function calls) | Piecewise rational approximation + refinement (2-4 calls) |
| **Speed** | Baseline | **~17Ã— faster** |
| **Accuracy** | High but slow | â‰¤10â»Â¹âµ error (exceeds double precision needs) |
| **Parallelization** | Sequential | **~6Ã— speedup** with OpenMP |

### Technical Approach

1. **Piecewise Rational Approximations**: Uses different polynomial ratios for central vs. tail regions
2. **Horner's Method**: Efficient polynomial evaluation
3. **Halley Refinement**: 0-2 iterations of third-order convergence to polish accuracy
4. **Numerical Stability**: Special formulas for extreme tails to avoid catastrophic cancellation

## ğŸ“Š Performance Results

### Algorithmic Improvement
```
Bisection (10â· calls):     8620 ms  â†’  862 ns/call
This implementation:        490 ms  â†’   49 ns/call  [~17Ã— faster]
```

### Parallelization Gain
```
Single-threaded (10â¶ calls):  47.3 ms  â†’  47.3 ns/call
OpenMP parallel:               7.2 ms  â†’   7.2 ns/call  [~6Ã— faster]
```

**Combined**: Up to **~100Ã— faster** than baseline for large batches on multi-core systems.

## ğŸ“ Accuracy Validation

| Range | Max Error |
|-------|-----------|
| **After initial fit** | ~10â»âµ |
| **After Halley refinement** | ~10â»Â¹âµ |

### Validated Properties
- âœ… **Symmetry**: Î¦â»Â¹(1-x) = -Î¦â»Â¹(x)
- âœ… **Monotonicity**: Strictly increasing (within 100 ULP margin)
- âœ… **Derivative sanity**: d/dx Î¦â»Â¹(x) = 1/Ï†(Î¦â»Â¹(x))
- âœ… **Round-trip accuracy**: Î¦(Î¦â»Â¹(x)) â‰ˆ x within machine precision

Tested range: [10â»Â¹Â², 1 - 10â»Â¹Â²]

## ğŸš€ Quick Start

### Prerequisites
```bash
# For visualization (optional)
python3 -m venv venv && source venv/bin/activate
pip install numpy scipy matplotlib

# For validation tests (optional)
apt install libboost-all-dev  # Boost library for reference calculations
```

### Build & Run

```bash
# Performance benchmarks
make probit_singlecore probit_parallel
./probit_singlecore && ./probit_parallel

# Validation tests
make probit_validation
./probit_validation

# View fitting analysis (optional)
python3 fitting_center.py
python3 fitting_tail.py
```

## ğŸ“ Technical Details

### Architecture

The implementation uses a **piecewise approach**:

1. **Central region** [0.02, 0.98]: 8/8 degree rational function
   - Fitted on 2000 Chebyshev-like nodes
   - Optimized for the bulk of probability mass

2. **Tail regions** (x < 0.02 or x > 0.98): Separate 8/8 degree rational
   - Fitted on 200 log-spaced + linear nodes
   - Uses transformed variable t = âˆš(-2 log m) for stability

3. **Halley refinement**: 0-2 adaptive iterations
   - Stable residual calculation using `expm1(log(Q(z)) - log(y))` in tails
   - Direct residual in central region

### Error Analysis

| Region | Mean Error | 99th %ile | Max Error |
|--------|------------|-----------|-----------|
| Center (post-fit) | 5.06Ã—10â»â· | 8.28Ã—10â»â¶ | 2.87Ã—10â»âµ |
| Tail (post-fit) | 1.78Ã—10â»âµ | 4.07Ã—10â»âµ | 4.44Ã—10â»âµ |
| **Full range (post-Halley)** | **1.32Ã—10â»Â¹â¶** | **5.22Ã—10â»Â¹â¶** | **1.51Ã—10â»Â¹âµ** |

## ğŸ”§ Usage Example

```cpp
#include "InverseCumulativeNormal.hpp"

// Standard normal (Î¼=0, Ïƒ=1)
quant::InverseCumulativeNormal probit;
double z = probit(0.975);  // Returns ~1.96 (95% confidence level)

// Custom distribution
quant::InverseCumulativeNormal custom(64.53, 3.05);  // Î¼=64.53, Ïƒ=3.05
double x = custom(0.75);  // Returns ~66.59

// Vectorized call (parallelized)
std::vector<double> probs = {0.1, 0.5, 0.9};
std::vector<double> results(probs.size());
probit(probs.data(), results.data(), probs.size());
```

## ğŸ“ Implementation Notes

- **Header-only**: Single `.hpp` file, no linking required
- **No external dependencies** for core functionality (uses standard `<cmath>`)
- **IEEE 754 compliant**: Handles subnormals, infinities gracefully
- **Thread-safe**: All methods are `const` and stateless per call

### Known Limitations

1. **Valid range**: [10â»Â¹Â², 1 - 10â»Â¹Â²] (machine precision limits below this)
2. **Monotonicity margin**: Guaranteed for inputs â‰¥100 ULP apart (numerical noise floor)
3. **Parallel speedup**: Hardware-dependent (measured on modern x86-64)

## ğŸ“š For More Details

See **[DESIGN.md](DESIGN.md)** for:
- Coefficient derivation process
- Ridge regularization strategy (Î» tuning)
- Numerical stability analysis
- Condition number handling in ill-conditioned tail fit

## ğŸ† Project Goals Achieved

| Objective | Target | Result | Status |
|-----------|--------|--------|--------|
| Accuracy (w/ refinement) | â‰¤10â»Â¹â° | ~10â»Â¹âµ | âœ… **Exceeded** |
| Scalar speedup | â‰¥10Ã— | ~17Ã— | âœ… **Exceeded** |
| Vector speedup | â‰¥1.5Ã— | ~6Ã— | âœ… **Exceeded** |
| Symmetry | Exact | âœ“ | âœ… |
| Monotonicity | Strict | âœ“ (within 100 ULP) | âœ… |

---


**Context**: OP Kiitorata Trainee Program Assignment
